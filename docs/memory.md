# How Memory Works

ntrp's memory system gives an AI assistant persistent, long-term memory across conversations. It stores raw facts, discovers patterns through consolidation, and retrieves relevant context at query time.

## Data Model

```
facts ──→ observations
  │            │
  └── entities ┘
       (links)

dreams (separate, cross-domain)
```

**Facts** are atomic pieces of information. "Alice joined the backend team in March." Each fact has an embedding, timestamps (`created_at`, `happened_at`), and access tracking.

**Entities** are named things extracted from facts (people, projects, places). Entity refs link facts to entities, forming a graph. This graph powers entity-based expansion during recall.

**Observations** are higher-level patterns synthesized from multiple facts. "Alice has transitioned from mobile to backend engineering." They track their source facts, change history, and evidence count.

**Dreams** are cross-domain insights generated by finding structural patterns between unrelated fact clusters. They live in a separate table and don't feed back into the recall pipeline.

## Write Path

When a fact is stored:

1. **Embed** the text (vector embedding)
2. **Extract entities** via LLM (proper nouns only: people, orgs, projects, places)
3. **Link** entities to the fact via `entity_refs`
4. **Queue** for consolidation (`consolidated_at = NULL`)

## Consolidation (Background Loop)

A background loop runs every 30 seconds, processing unconsolidated facts:

### Per-Fact Consolidation

For each pending fact:
1. Search for similar existing observations (vector search, top 5)
2. Fetch source facts for each candidate observation (with timestamps, sorted chronologically)
3. Ask LLM to decide: **update** an existing observation, **create** a new one, or **skip**
4. LLM sees temporal context (observation change history, fact `happened_at` timestamps) to handle transitions correctly
5. Apply the decision: re-embed updated observations, record history entries

The LLM is instructed to produce observations at a *higher abstraction level* than source facts -- patterns, trajectories, inferences -- not rephrases.

### Temporal Pass (Daily)

Scans for entities with 3+ recent facts and looks for temporal patterns no single fact captures:

- **Trends**: values changing over time (declining sleep, increasing workload)
- **Transitions**: role/state changes across facts
- **Cycles**: recurring patterns
- **Correlations**: co-occurring changes across domains

Before creating a new observation, checks if a very similar one already exists (cosine > 0.90). If so, enriches the existing observation with the new source facts instead of creating a duplicate.

### Observation Merge (Daily)

Deduplicates observations that became too similar over time. Uses recursive pairwise merging:

1. Load all observations with embeddings
2. Find the most similar pair above 0.90 cosine threshold
3. Ask LLM: merge into one, or skip (genuinely distinct)
4. If merge: update the keeper (higher evidence count), re-embed, delete the other
5. Repeat until no pairs remain above threshold

This is hierarchical agglomerative clustering with an LLM as the merge function. The recursive approach naturally handles clusters of any size without transitive chaining (merging A+B doesn't force merging the result with C unless they're actually similar after re-embedding).

### Dream Pass (Daily)

Generates cross-domain insights:

1. K-means cluster all facts by embedding similarity
2. For each cluster pair, extract core facts and supporters
3. LLM finds structural patterns, hidden dependencies, or ironic contradictions between domains
4. A separate evaluator LLM filters candidates, keeping only 1-3 that would genuinely surprise the user
5. Survivors are stored as dreams

### Execution Order

```
every 30s:  consolidate pending facts
daily:      temporal pass → observation merge → dream pass
```

## Read Path (Recall)

When the assistant needs context for a query:

1. **Hybrid search**: vector search + full-text search over facts, merged with Reciprocal Rank Fusion
2. **Entity expansion**: one-hop graph traversal from seed facts through shared entities, weighted by IDF (rare entities create stronger connections)
3. **Temporal expansion**: if query has a time context, fetch temporally proximate facts and filter by vector similarity
4. **Reranking**: cross-encoder reranker (optional, ZeroEntropy zerank-2) rescores all candidates
5. **Observation search**: vector search over observations, scored with decay and recency
6. **Decay + recency**: all results are weighted by access-based decay (`0.99^(hours/strength)`) and event recency (`exp(-hours/72)`)
7. **Format**: facts and observations are assembled into a context string injected into the system prompt

Observations with change history include their most recent transitions:
```
- Alice leads backend rewrite (previously: "Alice is the mobile app lead", changed Feb 2026)
```

## Storage

SQLite with WAL mode. Vector search via `sqlite-vec` (cosine distance). Full-text search via FTS5. All tables in a single `memory.db`.

## File Map

```
ntrp/memory/
  facts.py           FactMemory: main interface, consolidation loop
  consolidation.py   per-fact LLM consolidation into observations
  temporal.py         temporal pattern detection pass
  observation_merge.py  recursive pairwise deduplication
  dreams.py           cross-domain insight generation
  extraction.py       entity extraction from facts
  formatting.py       context string assembly for system prompt
  decay.py            access-based decay + recency boost
  models.py           Fact, Observation, Dream, Entity, etc.
  prompts.py          all LLM prompts
  store/
    base.py           schema, GraphDatabase
    facts.py          FactRepository (CRUD, search, entity graph)
    observations.py   ObservationRepository (CRUD, search, merge)
    dreams.py         DreamRepository
    retrieval.py      hybrid search, entity expansion, scoring
